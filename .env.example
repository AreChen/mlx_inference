# Authorization
API_KEY=your_api_key

# Set alias for local or Hugging Face model
MODEL_ALIASES={"Kimi-VL-A3B-Instruct-4bit":"/your_local_path/Kimi-VL-A3B-Instruct-4bit","DeepCoder":"mlx-community/DeepCoder-14B-Preview-8bit"}

# Preloading LLM/VLM model
# Load only one model at a time
# calling multiple models simultaneously can cause GPU resource conflicts.
PRELOAD_TEXT_MODELS=["mlx-community/DeepCoder-14B-Preview-8bit"]
PRELOAD_VISION_MODELS=["/Users/eapil/git/Models/Kimi-VL-A3B-Instruct-4bit"]

# Set an initial maximum token limit
MAX_TOKENS_TEXT=64000
MAX_TOKENS_VISION=64000

# Only applicable to some special models such as Kimi-VL-A3B-Think
SKIP_SPECIAL_TOKENS=False
